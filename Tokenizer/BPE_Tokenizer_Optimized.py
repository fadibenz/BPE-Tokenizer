import osimport regex as refrom collections import Counter, defaultdictfrom Tokenizer.pre_tokenization_chunks import find_chunk_boundariesimport multiprocessing as mpfrom pathlib import Pathdef pre_tokenization(training_data, special_tokens):    # This is taken from github.com/openai/tiktoken/pull/234/files (GPT2)    PAT = r"""'(?:[sdmt]|ll|ve|re)| ?\p{N}+| ?\p{L}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""    if not special_tokens:        return [m.group() for m in re.finditer(PAT, training_data)]    escaped_list = [re.escape(special_token) for special_token in special_tokens]    split_PAT = r"({})".format("|".join(escaped_list))    split_corpus = re.split(split_PAT, training_data)    pretokenized_train_data = []    for segment in split_corpus:        token_list = [special_token for special_token in special_tokens if special_token in segment]        if not segment:            continue        if len(token_list) >= 1:            pretokenized_train_data.append(segment)        else:            for m in re.finditer(PAT, segment):                pretokenized_train_data.append(m.group())    return pretokenized_train_datadef process_chunk(file_path, start, end, special_tokens):    with open(file_path, "rb") as f:        f.seek(start)        text = f.read(end - start).decode("utf-8",                                          errors="surrogateescape")        tokens = pre_tokenization(text, special_tokens)        return Counter(tokens)def get_best_pair(potential_merges: dict, token_str: dict):    max_freq = max(potential_merges.values())    candidate_pairs = [pair for pair, freq in potential_merges.items() if freq == max_freq]    best_pair = max(        candidate_pairs,        key=lambda pair: (token_str[pair[0]], token_str[pair[1]]))    return best_pairdef word_merge(word_tuple, best_pair, new_token_id):    new_word_list = []    k = 0    while k < len(word_tuple):        if k < len(word_tuple) - 1 and (word_tuple[k], word_tuple[k + 1]) == best_pair:            new_word_list.append(new_token_id)            k += 2        else:            new_word_list.append(word_tuple[k])            k += 1    new_word_tuple = tuple(new_word_list)    return new_word_tupledef decrement_counts(word_tuple, potential_merges, bigram_locations, count):    for j in range(len(word_tuple) - 1):        bigram = (word_tuple[j], word_tuple[j + 1])        potential_merges[bigram] -= count        if bigram in bigram_locations and word_tuple in bigram_locations[bigram]:            bigram_locations[bigram][word_tuple].discard(j)            if not bigram_locations[bigram][word_tuple]:                del bigram_locations[bigram][word_tuple]        if potential_merges[bigram] <= 0:            del potential_merges[bigram]def increment_counts(new_word_tuple, potential_merges, bigram_locations, count):    for j in range(len(new_word_tuple) - 1):        bigram = (new_word_tuple[j], new_word_tuple[j + 1])        potential_merges[bigram] += count        bigram_locations[bigram][new_word_tuple].add(j)def train_bpe(file_path: str | Path,              vocab_size: int,              special_tokens: list[str],              ):    merges = []    bigram_locations = defaultdict(lambda: defaultdict(set))    word_counts = Counter()    if not isinstance(special_tokens, list):        special_tokens = []    # reading file    with open(file_path, 'rb') as f:        boundaries = find_chunk_boundaries(f,                                           desired_num_chunks=os.cpu_count(),                                           split_special_token=b"<|endoftext|>")    chunks = [(file_path, start, end, special_tokens) for start, end in zip(boundaries[:-1], boundaries[1:])]    with mp.Pool(processes=mp.cpu_count()) as pool:        results = pool.starmap(process_chunk, chunks)    for result in results:        word_counts.update(result)    vocab = {i: bytes([i]) for i in range(256)}    token_str = {i: chr(i) for i in range(256)}    for i, token in enumerate(special_tokens):        vocab[256 + i] = token.encode("utf-8")    constructed_vocab = {}    for word, count in word_counts.items():        if word in special_tokens:            continue        constructed_vocab[tuple(word.encode("utf-8"))] = count    potential_merges = defaultdict(int)    for word_tuple, count in constructed_vocab.items():        for i in range(len(word_tuple) - 1):            bigram = (word_tuple[i], word_tuple[i + 1])            potential_merges[bigram] += count            bigram_locations[bigram][word_tuple].add(i)    # --- The Merging Loop ---    num_merges = vocab_size - len(vocab)    for i in range(num_merges):        if not potential_merges:            print("No more pairs to merge. Stopping early.")            break        best_pair = get_best_pair(potential_merges, token_str)        new_token_id = 256 + len(special_tokens) + i        merges.append(best_pair)        vocab[new_token_id] = vocab[best_pair[0]] + vocab[best_pair[1]]        token_str[new_token_id] = token_str[best_pair[0]] + token_str[best_pair[1]]        words_affected = list(bigram_locations[best_pair].keys())        for word_tuple in words_affected:            if word_tuple not in constructed_vocab:                continue            count = constructed_vocab[word_tuple]            # Decrement stats for all bigrams in the OLD word.            decrement_counts(word_tuple, potential_merges, bigram_locations, count)            # Merging            new_word_tuple = word_merge(word_tuple, best_pair, new_token_id)            constructed_vocab[new_word_tuple] = count            # Increment stats for all bigrams in the NEW word.            increment_counts(new_word_tuple, potential_merges, bigram_locations, count)            del constructed_vocab[word_tuple]        # Clean up the best_pair from bigram_locations (it should already be empty, but just to be safe)        if best_pair in bigram_locations:            del bigram_locations[best_pair]    readable_merges = [(vocab[p1], vocab[p2]) for p1, p2 in merges]    return vocab, readable_merges